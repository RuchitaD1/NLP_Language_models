{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuchitaD1/NLP_Language_models/blob/master/Untitled6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfdvnzEC5Y18",
        "colab_type": "text"
      },
      "source": [
        "We will start with a very basic n-gram language model. Open a new file ngram.py and write a generator function get ngrams(n, text), where n is an int that tells you the size of the n-grams, and text is a list of words/strings (if you don’t know what a generator function is, look up the yield keyword). The function’s output should be n-gram tuples of the form (word, context), where word is a string and context is a tuple of the n-1 preceding words/strings. Make sure to pad text with enough start tokens ‘\\<s>’ to be able to make n-grams for the first n-1 words; also make sure to add stop token ‘\\</s>’ (we will need it in Part 4).\n",
        "Next, define a class NGramLM. Write its initialization method init (self, n), which saves the int n and initializes three internal variables:\n",
        "• self.ngram counts for n-grams seen in the training data,\n",
        "• self.context counts for contexts seen in the training data,\n",
        "• self.vocabulary for keeping track of words seen in the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5lCtSlW3uDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ngrams(n, sentences):\n",
        "  \n",
        "  for sentence in sentences:\n",
        "    for i in range(len(sentence)-n+1):\n",
        "      temp_gram = []\n",
        "      for j in range(n):\n",
        "        temp_gram.append(sentence[i+j])\n",
        "      yield temp_gram\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36eqjY_s4GTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_ngramlm(n, corpus_path):\n",
        "  sentences = []\n",
        "  f = open(corpus_path, 'r')\n",
        "  \n",
        "  x=f.readlines()\n",
        "  model = NGramLM(n)\n",
        "  model.vocabulary['<s>']=0\n",
        "  model.vocabulary['</s>']=0\n",
        "  for line in x:\n",
        "      line = re.sub(r\"[*'*]\",\" \", line)\n",
        "      words = line.split()\n",
        "      arr = []\n",
        "      for _ in range(n-1):\n",
        "        arr.append('<s>')\n",
        "        model.vocabulary['<s>']+=n-1\n",
        "      for word in words:\n",
        "          word = word.lower()\n",
        "          cont=re.sub(r'[,-?:!]',\" \",word)\n",
        "          cont = cont.strip()\n",
        "\n",
        "          cont=re.sub(r'[^\\w\\s]', '', cont)\n",
        "\n",
        "          tokens=re.sub(r'[0-9]+', ' ', cont)\n",
        "          word=tokens.strip(\"\\n\")\n",
        "          if word in model.vocabulary:\n",
        "              model.vocabulary[word] = model.vocabulary[word]+1\n",
        "          else:\n",
        "              model.vocabulary[word] = 1\n",
        "          arr.append(word)\n",
        "      for _ in range(n-1):\n",
        "        arr.append('</s>')\n",
        "        model.vocabulary['</s>']+=n-1\n",
        "      sentences.append(arr)\n",
        "\n",
        "  ngrams = []\n",
        "  for tg in get_ngrams(n, sentences):\n",
        "      ngrams.append(tg) \n",
        "\n",
        "  for val in ngrams:\n",
        "\n",
        "    word = val[n-1]\n",
        "    context = tuple(val[:n-1])\n",
        "    if context in model.context_counts:\n",
        "        model.context_counts[context] = model.context_counts[context] + 1\n",
        "    else:\n",
        "        model.context_counts[context] = 1\n",
        "    val1 = (word,context)\n",
        "    if val1 in model.ngram_counts:\n",
        "      model.ngram_counts[val1] = model.ngram_counts[val1] + 1\n",
        "    else:\n",
        "      model.ngram_counts[val1] = 1\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNBnJL8-6VE7",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TXLTTFi6WEa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramLM:\n",
        "  def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.ngram_counts = {}\n",
        "        self.context_counts = {}\n",
        "        self.vocabulary = {}\n",
        "        \n",
        "        \n",
        "  def update(self, text):\n",
        "      text = re.sub(r\"[*'*]\",\" \", text)\n",
        "      words = text.split()\n",
        "      arr = []\n",
        "      for word in words:\n",
        "          word = word.lower()\n",
        "          cont=re.sub(r'[,-?:!]',\" \",word)\n",
        "          cont = cont.strip()\n",
        "\n",
        "          cont=re.sub(r'[^\\w\\s]', '', cont)\n",
        "\n",
        "          tokens=re.sub(r'[0-9]+', ' ', cont)\n",
        "          word=tokens.strip(\"\\n\")\n",
        "          if word in self.vocabulary:\n",
        "              self.vocabulary[word] = self.vocabulary[word]+1\n",
        "          else:\n",
        "              self.vocabulary[word] = 1\n",
        "          arr.append(word)\n",
        "      ngrams = []\n",
        "      for tg in get_ngrams(n, arr):\n",
        "          ngrams.append(tg)          \n",
        "      for val in ngrams:\n",
        "        word = val[self.n-1]\n",
        "        context = tuple(val[:self.n-1])\n",
        "        if context in model.context_counts:\n",
        "            model.context_counts[context] = model.context_counts[context] + 1\n",
        "        else:\n",
        "            model.context_counts[context] = 1\n",
        "        \n",
        "        if val in model.ngram_counts:\n",
        "          model.ngram_counts[val] = model.ngram_counts[val] + 1\n",
        "        else:\n",
        "          model.ngram_counts[val] = 1 \n",
        "\n",
        "          \n",
        "          \n",
        "  def word_prob(self, word, context, delta=0):\n",
        "    word = word.lower()\n",
        "    if context not in self.context_counts:\n",
        "      return 1.0/float(len(self.vocabulary))\n",
        "    if (word, context) not in self.ngram_counts:\n",
        "      c_context = self.context_counts[context]\n",
        "      ng = ('<unk>',context)\n",
        "      c_ngram = self.vocabulary['<unk>']\n",
        "      wp = (float(c_ngram)+float(delta))/(float(c_context)+(float(delta)*float(len(self.vocabulary))))\n",
        "      #print(c_context, c_ngram)\n",
        "      return wp\n",
        "    c_context = self.context_counts[context]\n",
        "    ng=(word,context)\n",
        "    c_ngram = self.ngram_counts[ng]\n",
        "    \n",
        "    wp = (float(c_ngram)+float(delta))/(float(c_context)+(float(delta)*float(len(self.vocabulary))))\n",
        "    #print(wp)\n",
        "    return wp\n",
        "  \n",
        "                 \n",
        "  def random_word(self, context, delta=0):\n",
        "    r = random.random()\n",
        "    print('r',r)\n",
        "    word2probs = {}\n",
        "    probs = []\n",
        "    ws=[]\n",
        "    for word in sorted(self.vocabulary):\n",
        "      wp = self.word_prob(word, context, delta)\n",
        "      word2probs[word]=wp\n",
        "      ws.append(word)\n",
        "      probs.append(wp)\n",
        "    p = 1.0\n",
        "    psum = 0.0\n",
        "    #print('sum',sum(probs))\n",
        "    flag = 0\n",
        "    i = 0\n",
        "    for prob in sorted(probs):\n",
        "      #print(set(probs))\n",
        "      if psum>r and flag==0:\n",
        "        p = prob\n",
        "        #print('sum reached',psum,prob)\n",
        "        flag=1\n",
        "        break\n",
        "      else:\n",
        "        psum+=prob\n",
        "    for key in word2probs:\n",
        "      if word2probs[key] == p:\n",
        "        #print('probability at',p)\n",
        "        return key\n",
        "      \n",
        "  def likeliest_word(self, context, delta=0):\n",
        "\n",
        "    max_prob = -0.1\n",
        "    l_word=''\n",
        "    for word in self.vocabulary.keys():\n",
        "        wp = self.word_prob(word, context, delta)\n",
        "\n",
        "        if(wp >= max_prob):\n",
        "            max_prob = wp\n",
        "            l_word = word\n",
        "    return l_word\n",
        "        \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYHyWGUN6XH6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_ngramlmunk(n, corpus_path):\n",
        "  sentences = []\n",
        "  \n",
        " \n",
        "\n",
        "  f = open(corpus_path, 'r')\n",
        "  x=f.readlines()\n",
        "  model = NGramLM(n)\n",
        "  rare_words = mask_rare(corpus_path)\n",
        "  model.vocabulary['<s>']=0\n",
        "  model.vocabulary['</s>']=0\n",
        "  model.vocabulary['<unk>']=0\n",
        "  for line in x:\n",
        "      line = re.sub(r\"[*'*]\",\" \", line)\n",
        "      words = line.split()\n",
        "      if len(words) == 0:\n",
        "        continue\n",
        "      arr = []\n",
        "      for _ in range(n-1):\n",
        "        arr.append('<s>')\n",
        "        model.vocabulary['<s>']+=n-1\n",
        "      for word in words:\n",
        "          word = word.lower()\n",
        "          cont=re.sub(r'[,-?:!]',\" \",word)\n",
        "          cont = cont.strip()\n",
        "\n",
        "          cont=re.sub(r'[^\\w\\s]', '', cont)\n",
        "\n",
        "          tokens=re.sub(r'[0-9]+', ' ', cont)\n",
        "          word=tokens.strip(\"\\n\")\n",
        "          if word in model.vocabulary:\n",
        "              model.vocabulary[word] = model.vocabulary[word]+1\n",
        "          else:\n",
        "              model.vocabulary[word] = 1\n",
        "          if word in rare_words:\n",
        "            arr.append('<unk>')\n",
        "            model.vocabulary['<unk>']+=1\n",
        "            model.vocabulary.pop(word)\n",
        "          else:\n",
        "            arr.append(word)\n",
        "      for _ in range(n-1):\n",
        "        arr.append('</s>')\n",
        "        model.vocabulary['</s>']+=n-1\n",
        "      sentences.append(arr)\n",
        "      \n",
        "  \n",
        "\n",
        "  ngrams = []\n",
        "  for tg in get_ngrams(n, sentences):\n",
        "      ngrams.append(tg) \n",
        "\n",
        "  for val in ngrams:\n",
        "\n",
        "    word = val[n-1]\n",
        "    context = tuple(val[:n-1])\n",
        "    if context in model.context_counts:\n",
        "        model.context_counts[context] = model.context_counts[context] + 1\n",
        "    else:\n",
        "        model.context_counts[context] = 1\n",
        "    val1 = (word,context)\n",
        "    if val1 in model.ngram_counts:\n",
        "      model.ngram_counts[val1] = model.ngram_counts[val1] + 1\n",
        "    else:\n",
        "      model.ngram_counts[val1] = 1\n",
        "  return model\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJOmu6bH4WNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQIztYZh4anA",
        "colab_type": "text"
      },
      "source": [
        "Writeup Question 1.1: What data types did you use for the two counters and the vocabulary, and did you initialize the vocabulary to be empty or already containing some token(s)? Explain why. You may want to wait to answer this question until after you complete the programming parts, in case you change your mind.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3Jxc8Go4gTg",
        "colab_type": "text"
      },
      "source": [
        "*Data types used for the 2 counters and the vocabulary are dictionaries. The vocabulary is initialized with count of start tokens \\<s> and stop tokens \\</s> initiated to value 0. This is done so that they can be easily updated with the count later.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6HgzMKU4dnO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zjzgAn16G-F",
        "colab_type": "text"
      },
      "source": [
        "Add a method update(self, text) that updates the NGramLM’s internal counts and vocabulary for the n-grams in text, which is again a list of words/strings.\n",
        "Now write a function create ngramlm(n, corpus path) that returns an NGramLM trained on the data in the file corpus path. This is not a word tokenization homework, so simply use split() to tokenize lines using whitespace.\n",
        "          \n",
        "Now that we can train a model, we need to be able to use it to predict word and sentence probabilities. Write a method word prob(self, word, context) that returns the prob- ability of the n-gram (word, context) using the model’s internal counters; the output should be a float. If context is previously unseen (ie. not in the training data), the probability should be 1/|V |, where V is the model’s vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1JTeoLS6Hn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTkiJnTS6nT5",
        "colab_type": "text"
      },
      "source": [
        "Writeup Question 1.2: Why do we have a special case for unseen contexts? Why do we set the probability to be 1/|V |?\n",
        "To predict the probability of a sentence, we multiply together its n-gram probabilities. This can be a very small number, so to avoid underflow, we will report the sentence’s log probability instead. Import the math library and write a function text prob(model, text) that returns the log probability of text, which is again a list of words/strings, under model, which is a trained NGramLM. The choice of base for the log doesn’t matter as long as it’s consistent with the base you use for perplexity in Part 3, so we will just use the library’s default base e. The output of this function should be a (negative) float.\n",
        "We are now ready to predict the probability of a sentence. Train a trigram NGramLM on warpeace.txt and use it to predict the probabilities of the following sentences:\n",
        "• God has given it to me, let him who touches it beware! • Where is the prince, my Dauphin?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCXTL8Ca6n5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhfW0eT26pAA",
        "colab_type": "text"
      },
      "source": [
        "*To get the probability of a word given a context, we divide by the count of the context. If the context doesnt exist, its count will become 0. Which will lead to an error. Least probability of a word is 1/|V|. Thus, we set the probability of unseen contexts to 1/|V|*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPSrTWZ17mdo",
        "colab_type": "code",
        "outputId": "c340e373-42f4-4a6b-d4b9-b86f94496101",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "model_wp = create_ngramlm(3, 'warpeace.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-3051ff822989>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_wp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_ngramlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'warpeace.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-2-75f51c945e49>\u001b[0m in \u001b[0;36mcreate_ngramlm\u001b[0;34m(n, corpus_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcreate_ngramlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorpus_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'warpeace.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHENYquu78k7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def text_prob(model, text, delta=0):\n",
        "  text = re.sub(r\"[*'*]\",\" \", text)\n",
        "  words = text.split()\n",
        "  arr = []\n",
        "  for _ in range(model.n-1):\n",
        "    arr.append('<s>')\n",
        "  for word in words:\n",
        "    word = word.lower()\n",
        "    cont=re.sub(r'[,-?:!]',\" \",word)\n",
        "    cont = cont.strip()\n",
        "\n",
        "    cont=re.sub(r'[^\\w\\s]', '', cont)\n",
        "\n",
        "    tokens=re.sub(r'[0-9]+', ' ', cont)\n",
        "    word=tokens.strip(\"\\n\")\n",
        "    arr.append(word)\n",
        "  for _ in range(model.n-1):\n",
        "    arr.append('</s>')\n",
        "  p = 0.0\n",
        "  arr = [arr]\n",
        " \n",
        "  for ng in get_ngrams(model.n, arr):\n",
        "    wd = ng[model.n-1]\n",
        "    ctx = tuple(ng[:model.n-1])\n",
        "    wp = model.word_prob(wd, ctx, delta)\n",
        "    p+= math.log(wp)\n",
        "  return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WnYw41F8LSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_prob(model_wp, 'God has given it to me, let him who touches it beware!')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_Dbze5l8SKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_prob(model_wp, 'Where is the prince, my Dauphin?')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fq9-rUxP8YvK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTzJwFIo9bUW",
        "colab_type": "text"
      },
      "source": [
        "Writeup Question 1.3: What are your model’s predicted probabilities for these two sentences? Did anything unusual happen when you ran the second sentence? Explain what happened and why. (If you’re not sure or can’t remember from what we talked about in class, try stepping through your code to see what’s going on.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LHkGWU1b9bub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2Yu_dTp9db6",
        "colab_type": "text"
      },
      "source": [
        "The model's predicted probabilities for the first sentence is -30.515.\n",
        "The model throws an error on the second sentence because the n gram doesnt exist for the given values. SPecifically, word = prince and context =(is,the)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAbTQiFT-TL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mask_rare(corpus):\n",
        "  f = open(corpus, 'r')\n",
        "  text = f.readlines()\n",
        "  vocab={}\n",
        "  for line in text:\n",
        "    line = re.sub(r\"[*'*]\",\" \", line)\n",
        "    words = line.split()\n",
        "    for word in words:\n",
        "        word = word.lower()\n",
        "        cont=re.sub(r'[,-?:!]',\" \",word)\n",
        "        cont = cont.strip()\n",
        "\n",
        "        cont=re.sub(r'[^\\w\\s]', '', cont)\n",
        "\n",
        "        tokens=re.sub(r'[0-9]+', ' ', cont)\n",
        "        word=tokens.strip(\"\\n\")\n",
        "        if word in vocab:\n",
        "            vocab[word] = vocab[word]+1\n",
        "        else:\n",
        "            vocab[word] = 1  \n",
        "  rare_words = []\n",
        "  for word in vocab:\n",
        "    if vocab[word] == 1:\n",
        "      rare_words.append(word)\n",
        "  \n",
        "  return rare_words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlfXz5Kp90zP",
        "colab_type": "code",
        "outputId": "dfaac0d8-da67-489f-eb5a-cc1444bd4853",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "model_wp_unk = create_ngramlmunk(3, 'warpeace.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-af23260ac8f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_wp_unk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_ngramlmunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'warpeace.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-58e1a6ac05d7>\u001b[0m in \u001b[0;36mcreate_ngramlmunk\u001b[0;34m(n, corpus_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGramLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'warpeace.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpUTZn5d-V-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tyuhayX-f6C",
        "colab_type": "text"
      },
      "source": [
        "Writeup Question 2.1: Try predicting the log probability of that second sentence again. Have we fixed whatever was going on? Why or why not?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB0D7jpX-gRm",
        "colab_type": "code",
        "outputId": "c7e9a40b-fd82-4165-8f72-6605dc1e488e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "text_prob(model_wp_unk, 'Where is the prince, my Dauphin?')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d7af11e79bd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wp_unk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'text_prob' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRfPTCLw-k5c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNbdgRmK-nHO",
        "colab_type": "text"
      },
      "source": [
        "*We can now predict the probability for the second sentence. The probability for the second sentence is -20.971. This is because, now the denominator is not zero in the formula for calculating the probability*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjW3Vws9_zgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJaZVQ1Z_-Ye",
        "colab_type": "text"
      },
      "source": [
        "Writeup Question 2.2: How did you modify the Laplace smoothing formula? Explain why the modification was necessary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5y8tKdyn_-qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COBlhb0h__Ww",
        "colab_type": "text"
      },
      "source": [
        "*No modification required*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMggqW1aADfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1snMlRMAIxl",
        "colab_type": "text"
      },
      "source": [
        "ry predicting the log probabilities of both sentences again using different values for delta. What do you get? How does the value of delta affect the predicted log probabilities? Based on these examples, do you think Laplace smoothing works well for n-gram language models? Why or why not?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bo1j2-wWAF1m",
        "colab_type": "code",
        "outputId": "85a2c229-b00b-48c4-d6f0-e15a22d5c0db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "print(text_prob(model_wp_unk, 'God has given it to me, let him who touches it beware!'))\n",
        "print(text_prob(model_wp_unk, 'God has given it to me, let him who touches it beware!', delta=0.001))\n",
        "print(text_prob(model_wp_unk, 'God has given it to me, let him who touches it beware!', delta=0.01))\n",
        "print(text_prob(model_wp_unk, 'God has given it to me, let him who touches it beware!', delta=0.1))\n",
        "print(text_prob(model_wp_unk, 'God has given it to me, let him who touches it beware!', delta=0.5))\n",
        "print(text_prob(model_wp_unk, 'God has given it to me, let him who touches it beware!', delta=0.9))\n",
        "print(text_prob(model_wp_unk, 'God has given it to me, let him who touches it beware!', delta=1.0))\n",
        "\n",
        "print(text_prob(model_wp, 'God has given it to me, let him who touches it beware!'))\n",
        "print(text_prob(model_wp, 'God has given it to me, let him who touches it beware!', delta=0.001))\n",
        "print(text_prob(model_wp, 'God has given it to me, let him who touches it beware!', delta=0.01))\n",
        "print(text_prob(model_wp, 'God has given it to me, let him who touches it beware!', delta=0.1))\n",
        "print(text_prob(model_wp, 'God has given it to me, let him who touches it beware!', delta=0.5))\n",
        "print(text_prob(model_wp, 'God has given it to me, let him who touches it beware!', delta=0.9))\n",
        "print(text_prob(model_wp, 'God has given it to me, let him who touches it beware!', delta=1.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-10b3d03ec8da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wp_unk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'God has given it to me, let him who touches it beware!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wp_unk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'God has given it to me, let him who touches it beware!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wp_unk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'God has given it to me, let him who touches it beware!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wp_unk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'God has given it to me, let him who touches it beware!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wp_unk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'God has given it to me, let him who touches it beware!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_prob' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aIvBA_TwBILQ",
        "colab_type": "code",
        "outputId": "200ca374-1daa-4caf-c07a-10bb2bbf3f74",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "print(text_prob(model_wp_unk, 'Where is the prince, my Dauphin?'))\n",
        "print(text_prob(model_wp_unk, 'Where is the prince, my Dauphin?', delta=0.001))\n",
        "print(text_prob(model_wp_unk, 'Where is the prince, my Dauphin?', delta=0.01))\n",
        "print(text_prob(model_wp_unk, 'Where is the prince, my Dauphin?', delta=0.1))\n",
        "print(text_prob(model_wp_unk, 'Where is the prince, my Dauphin?', delta=0.5))\n",
        "print(text_prob(model_wp_unk, 'Where is the prince, my Dauphin?', delta=0.9))\n",
        "print(text_prob(model_wp_unk, 'Where is the prince, my Dauphin?', delta=1.0))\n",
        "\n",
        "print(text_prob(model_wp, 'Where is the prince, my Dauphin?'))\n",
        "print(text_prob(model_wp, 'Where is the prince, my Dauphin?', delta=0.001))\n",
        "print(text_prob(model_wp, 'Where is the prince, my Dauphin?', delta=0.01))\n",
        "print(text_prob(model_wp, 'Where is the prince, my Dauphin?', delta=0.1))\n",
        "print(text_prob(model_wp, 'Where is the prince, my Dauphin?', delta=0.5))\n",
        "print(text_prob(model_wp, 'Where is the prince, my Dauphin?', delta=0.9))\n",
        "print(text_prob(model_wp, 'Where is the prince, my Dauphin?', delta=1.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-386bd3660f22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wp_unk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wp_unk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wp_unk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wp_unk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_wp_unk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_prob' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYeES_hjDXpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YicEv4fXDjxo",
        "colab_type": "text"
      },
      "source": [
        "Writeup Question 2.4: Train a trigram NGramInterpolator with lambdas = [0.33, 0.33, 0.33] and use it to predict the log probabilities of the two example sentences. What do you get? How does its compare with the base NGramLM, both with and without smoothing?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a5B99HZDiUl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramInterpolator:\n",
        "  def __init__(self, n, lambdas):\n",
        "    self.n = n\n",
        "    self.lambdas = lambdas\n",
        "    self.lms = []\n",
        "    bn = n\n",
        "    for l in self.lambdas:\n",
        "      ng = create_ngramlmunk(bn, 'warpeace.txt')\n",
        "      \n",
        "      bn-=1\n",
        "      self.lms.append(ng)\n",
        "      \n",
        "  def update(self, text):\n",
        "    for lm in self.lms:\n",
        "      lm.update(text)\n",
        "  \n",
        "  def word_prob(self, word, context, delta=0):\n",
        "    pfinal = 0.0\n",
        "    for i in range(len(self.lambdas)):\n",
        "      pfinal += self.lms[i].word_prob(word, context, delta)*self.lambdas[i]\n",
        "    return pfinal"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rdLpxv4kDoCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(filename):\n",
        "        arr = []\n",
        "        f = open(filename, 'r')\n",
        "    \n",
        "        x=f.readlines()\n",
        "        \n",
        "        for line in x:\n",
        "            \n",
        "            line = re.sub(r\"[*'*]\",\" \", line)\n",
        "            words = line.split()\n",
        "            for word in words:\n",
        "                word = word.lower()\n",
        "                cont=re.sub(r'[,-?:!]',\" \",word)\n",
        "                cont = cont.strip()\n",
        "\n",
        "                tokens=re.sub(r'[0-9]+', ' ', cont)\n",
        "                tokens=tokens.strip(\"\\n\")\n",
        "                \n",
        "                arr.append(tokens)\n",
        "        return arr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7AMWTnTdDruW",
        "colab_type": "code",
        "outputId": "f3e16e6d-d7e9-4bb3-e514-8e96987b746b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        }
      },
      "source": [
        "ngli = NGramInterpolator(3, [0.33, 0.33, 0.33])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-1fdafd92be27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mngli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGramInterpolator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.33\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.33\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-40b148d690b2>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, n, lambdas)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mbn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambdas\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m       \u001b[0mng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_ngramlmunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'warpeace.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m       \u001b[0mbn\u001b[0m\u001b[0;34m-=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-58e1a6ac05d7>\u001b[0m in \u001b[0;36mcreate_ngramlmunk\u001b[0;34m(n, corpus_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGramLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'warpeace.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6We047w7DuHz",
        "colab_type": "code",
        "outputId": "37f67469-7440-4456-d929-641c97ef5c57",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "print(text_prob(ngli, 'Where is the prince, my Dauphin?'))\n",
        "print(text_prob(ngli, 'Where is the prince, my Dauphin?', delta=0.001))\n",
        "print(text_prob(ngli, 'Where is the prince, my Dauphin?', delta=0.01))\n",
        "print(text_prob(ngli, 'Where is the prince, my Dauphin?', delta=0.1))\n",
        "print(text_prob(ngli, 'Where is the prince, my Dauphin?', delta=0.5))\n",
        "print(text_prob(ngli, 'Where is the prince, my Dauphin?', delta=0.9))\n",
        "print(text_prob(ngli, 'Where is the prince, my Dauphin?', delta=1.0))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-c049f7e5a05a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_prob' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOC_zU8eFkK9",
        "colab_type": "code",
        "outputId": "2be2c654-5fc0-4ca6-b8dd-267b73d009a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "print(text_prob(ngli, 'God has given it to me, let him who touches it beware!'))\n",
        "print(text_prob(ngli, 'God has given it to me, let him who touches it beware!', delta=0.001))\n",
        "print(text_prob(ngli, 'God has given it to me, let him who touches it beware!', delta=0.01))\n",
        "print(text_prob(ngli, 'God has given it to me, let him who touches it beware!', delta=0.1))\n",
        "print(text_prob(ngli, 'God has given it to me, let him who touches it beware!', delta=0.5))\n",
        "print(text_prob(ngli, 'God has given it to me, let him who touches it beware!', delta=0.9))\n",
        "print(text_prob(ngli, 'God has given it to me, let him who touches it beware!', delta=1.0))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0f3832c0d595>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'God has given it to me, let him who touches it beware!'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'God has given it to me, let him who touches it beware!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'God has given it to me, let him who touches it beware!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'God has given it to me, let him who touches it beware!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngli\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'God has given it to me, let him who touches it beware!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'text_prob' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pCsnhyBKV8s",
        "colab_type": "code",
        "outputId": "66e8c7aa-621a-4a95-e4c7-184879e8e76b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        }
      },
      "source": [
        "Write a function perplexity(model, corpus path) that returns the perplexity of a trained model on the test data in the file corpus path. You will need to load the test data from file, just like you loaded the training data (you don’t need to mask rare words, though), and you will need to count N, the total number of tokens in the test data. Make sure you are using the same base e as in Part 1."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-5f1944701726>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Write a function perplexity(model, corpus path) that returns the perplexity of a trained model on the test data in the file corpus path. You will need to load the test data from file, just like you loaded the training data (you don’t need to mask rare words, though), and you will need to count N, the total number of tokens in the test data. Make sure you are using the same base e as in Part 1.\u001b[0m\n\u001b[0m          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckOuvVlyKd6K",
        "colab_type": "text"
      },
      "source": [
        "Write a function perplexity(model, corpus path) that returns the perplexity of a trained model on the test data in the file corpus path. You will need to load the test data from file, just like you loaded the training data (you don’t need to mask rare words, though), and you will need to count N, the total number of tokens in the test data. Make sure you are using the same base e as in Part 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ0BqQiHKehp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def perplexity(model, corpus, delta=0):\n",
        "  arr = preprocess(corpus)\n",
        "  N = len(arr)\n",
        "  f = open(corpus, 'r')\n",
        "  x = f.readlines()\n",
        "  logp = 0.0\n",
        "  for line in x:\n",
        "    logp += text_prob(model, line, delta)\n",
        "  logp/=N\n",
        "  return 2**(-1*logp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxyArumBLr5W",
        "colab_type": "text"
      },
      "source": [
        "Writeup Question 3.1: Train two trigram NGramLMs on shakespeare.txt, one with smoothing (use delta = 0.5) and one without. (As you have probably noticed, the NGramInterpolator is slower because it builds multiple models, so we will be using plain NGramLMs for the rest of the homework.) Evaluate the two models’ perplexities using sonnets.txt as the test data. What do you get? Does anything unusual happen? Explain what and why.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2THJyMfKgiHE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89DPgGpDgiY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W00c0W1mLsdQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ng3_shk = create_ngramlmunk(3, 'shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg47BKPWgiAe",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwrO5DHxMOYz",
        "colab_type": "code",
        "outputId": "d241b5f5-565c-40a1-c406-b6ea90a6f891",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "perplexity(ng3_shk, 'sonnets.txt', delta = 0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9d07f74464b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng3_shk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sonnets.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ng3_shk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZCTC9CoMzHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRoGY72QM2FE",
        "colab_type": "text"
      },
      "source": [
        "Writeup Question 3.2: Evaluate the perplexities of a smoothed (delta = 0.5) tri- gram NGramLM trained on shakespeare.txt and one trained on warpeace.txt. Use sonnets.txt as the test data for both. What do you get? Which one performs better, and why do you think that’s the case?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxpTFUBnM2jv",
        "colab_type": "code",
        "outputId": "b4d0f79a-d7ce-477d-fcc4-2dc6c21acfc3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "perplexity(ng3_shk, 'sonnets.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-6729e9c5162f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng3_shk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sonnets.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ng3_shk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF5nQ7cHNH2a",
        "colab_type": "code",
        "outputId": "e1574ffc-6277-4880-eee8-4352c5e70350",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "ng3_wp = create_ngramlmunk(3, 'warpeace.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a62f296d1040>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mng3_wp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_ngramlmunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'warpeace.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-58e1a6ac05d7>\u001b[0m in \u001b[0;36mcreate_ngramlmunk\u001b[0;34m(n, corpus_path)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNGramLM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'warpeace.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdvmirbcNM6p",
        "colab_type": "code",
        "outputId": "f6521158-80bb-457e-a954-7a66e3d90eef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "perplexity(ng3_wp, 'sonnets.txt', delta=0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-96abfbe22ca1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng3_wp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sonnets.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ng3_wp' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "866oy42QN173",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "random.seed(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ofIxptwwN7tn",
        "colab_type": "text"
      },
      "source": [
        "Writeup Question 4.1: Train a trigram model on shakespeare.txt and generate 5 sentences with max length = 10. What did you generate? Are they good (Elizabethan) English sentences? What are some problems you see with the generated sentences?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6JlTYotHN8Lz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_text(model, max_length, delta=0):\n",
        "  if '' in model.vocabulary:\n",
        "    model.vocabulary.pop('')\n",
        "  if ' ' in model.vocabulary:\n",
        "    model.vocabulary.pop(\" \")\n",
        "    \n",
        "  generate_string = \"\"\n",
        "  context=[]\n",
        "  for _ in range(model.n-1):\n",
        "    context.append('<s>') \n",
        "  for i in range(max_length):\n",
        "\n",
        "    new_word=model.random_word(tuple(context))\n",
        "\n",
        "    if new_word == '</s>':\n",
        "      print('</s> recieved')\n",
        "      return generate_string\n",
        "    generate_string += \" \"+new_word\n",
        "    \n",
        "    context = context[1:]\n",
        "    context.append(new_word)\n",
        "    \n",
        "  return generate_string\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccszyGCVOBG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " def likeliest_text(model, max_length, delta=0):\n",
        "  if '' in model.vocabulary:\n",
        "    model.vocabulary.pop('')\n",
        "  if ' ' in model.vocabulary:\n",
        "    model.vocabulary.pop(\" \")\n",
        "    \n",
        "  generate_string = \"\"\n",
        "  context=[]\n",
        "  for _ in range(model.n-1):\n",
        "    context.append('<s>') \n",
        "  for i in range(max_length):\n",
        "      print(context)\n",
        "      new_word=model.likeliest_word(tuple(context))\n",
        "      print(new_word)\n",
        "      if new_word == '</s>':\n",
        "          print('</s>')\n",
        "          return generate_string\n",
        "      generate_string += \" \"+new_word\n",
        "      #print(context)\n",
        "      context = context[1:]\n",
        "      context.append(new_word)\n",
        "\n",
        "  return generate_string\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyue9QcYOMJH",
        "colab_type": "code",
        "outputId": "1c3ff372-d658-4f49-8ff0-7e05d9103e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "random_text(ng3_shk, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-1aac4f43de41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng3_shk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ng3_shk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOXktomCOciO",
        "colab_type": "code",
        "outputId": "91cd0a23-198c-449c-a520-067ff7148d46",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "random_text(ng3_shk, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-1aac4f43de41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng3_shk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ng3_shk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1J9vYYxO4dF",
        "colab_type": "code",
        "outputId": "85442818-bf3c-455c-ef2b-c58d70175942",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        }
      },
      "source": [
        "random_text(ng3_shk, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-1aac4f43de41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrandom_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng3_shk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ng3_shk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTNLlyKlO56K",
        "colab_type": "code",
        "outputId": "f0dae246-78cd-4de2-a193-c6b480004b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "random_text(ng3_shk, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r 0.13436424411240122\n",
            "r 0.8474337369372327\n",
            "</s> recieved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' commend'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKm9vgXvO7LY",
        "colab_type": "code",
        "outputId": "b74a0962-c434-4eca-e64c-16da60c9a25b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "random_text(ng3_shk, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r 0.763774618976614\n",
            "r 0.2550690257394217\n",
            "r 0.49543508709194095\n",
            "r 0.4494910647887381\n",
            "r 0.651592972722763\n",
            "</s> recieved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' you be acquainted <s>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEWBPGHtO8Oo",
        "colab_type": "code",
        "outputId": "00a926ff-95e0-4fa6-e354-5124469f07b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "random_text(ng3_shk, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "r 0.7887233511355132\n",
            "r 0.0938595867742349\n",
            "r 0.02834747652200631\n",
            "</s> recieved\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' for after'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbyBuXlRO9dz",
        "colab_type": "code",
        "outputId": "00f32fbd-eef2-438f-ac1c-74eaa942a37d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "likeliest_text(ng3_shk,10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', '<s>']\n",
            "goodwin\n",
            "['<s>', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin']\n",
            "goodwin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' goodwin goodwin goodwin goodwin goodwin goodwin goodwin goodwin goodwin goodwin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hiy7lU28TnK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ng2_shk = create_ngramlmunk(2, 'shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nO83eDicT3QQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ng4_shk = create_ngramlmunk(4, 'shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggfFOe9WUAw9",
        "colab_type": "code",
        "outputId": "5b1eb44e-ea4a-4ddc-faff-de10a2a30e83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "likeliest_text(ng2_shk, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>']\n",
            "goodwin\n",
            "['goodwin']\n",
            "goodwin\n",
            "['goodwin']\n",
            "goodwin\n",
            "['goodwin']\n",
            "goodwin\n",
            "['goodwin']\n",
            "goodwin\n",
            "['goodwin']\n",
            "goodwin\n",
            "['goodwin']\n",
            "goodwin\n",
            "['goodwin']\n",
            "goodwin\n",
            "['goodwin']\n",
            "goodwin\n",
            "['goodwin']\n",
            "goodwin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' goodwin goodwin goodwin goodwin goodwin goodwin goodwin goodwin goodwin goodwin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BiuUH5zBU_HQ",
        "colab_type": "code",
        "outputId": "196125d7-fc9c-42da-c6b7-557816d7487f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "likeliest_text(ng4_shk, 10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['<s>', '<s>', '<s>']\n",
            "goodwin\n",
            "['<s>', '<s>', 'goodwin']\n",
            "goodwin\n",
            "['<s>', 'goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin', 'goodwin']\n",
            "goodwin\n",
            "['goodwin', 'goodwin', 'goodwin']\n",
            "goodwin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' goodwin goodwin goodwin goodwin goodwin goodwin goodwin goodwin goodwin goodwin'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f2DWkztVDY_",
        "colab_type": "code",
        "outputId": "0fd26493-a894-4e19-e16f-925c48498eca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sorted(ng3_shk.vocabulary)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['</s>',\n",
              " '<s>',\n",
              " '<unk>',\n",
              " 'a',\n",
              " 'a bed',\n",
              " 'a birding',\n",
              " 'a bleeding',\n",
              " 'a cold',\n",
              " 'a day',\n",
              " 'a doing',\n",
              " 'a field',\n",
              " 'a foot',\n",
              " 'a hold',\n",
              " 'a hungry',\n",
              " 'a land',\n",
              " 'a making',\n",
              " 'a piece',\n",
              " 'a weary',\n",
              " 'a wooing',\n",
              " 'a work',\n",
              " 'aaron',\n",
              " 'abandon',\n",
              " 'abandoned',\n",
              " 'abase',\n",
              " 'abate',\n",
              " 'abated',\n",
              " 'abatement',\n",
              " 'abbess',\n",
              " 'abbey',\n",
              " 'abbot',\n",
              " 'abed',\n",
              " 'abel',\n",
              " 'abergavenny',\n",
              " 'abhor',\n",
              " 'abhorr',\n",
              " 'abhorred',\n",
              " 'abhorring',\n",
              " 'abhors',\n",
              " 'abhorson',\n",
              " 'abide',\n",
              " 'abides',\n",
              " 'abilities',\n",
              " 'ability',\n",
              " 'abject',\n",
              " 'abjects',\n",
              " 'abjure',\n",
              " 'abjured',\n",
              " 'able',\n",
              " 'aboard',\n",
              " 'abode',\n",
              " 'abominable',\n",
              " 'abortive',\n",
              " 'abound',\n",
              " 'about',\n",
              " 'above',\n",
              " 'abraham',\n",
              " 'abram',\n",
              " 'abreast',\n",
              " 'abridge',\n",
              " 'abridged',\n",
              " 'abridgement',\n",
              " 'abroach',\n",
              " 'abroad',\n",
              " 'absence',\n",
              " 'absent',\n",
              " 'absolute',\n",
              " 'absolutely',\n",
              " 'absolved',\n",
              " 'abstinence',\n",
              " 'abstract',\n",
              " 'absurd',\n",
              " 'abundance',\n",
              " 'abundant',\n",
              " 'abuse',\n",
              " 'abused',\n",
              " 'abuses',\n",
              " 'abusing',\n",
              " 'aby',\n",
              " 'abysm',\n",
              " 'accent',\n",
              " 'accents',\n",
              " 'accept',\n",
              " 'acceptance',\n",
              " 'accepted',\n",
              " 'access',\n",
              " 'accessary',\n",
              " 'accident',\n",
              " 'accidental',\n",
              " 'accidentally',\n",
              " 'accidents',\n",
              " 'accommodated',\n",
              " 'accompanied',\n",
              " 'accompany',\n",
              " 'accomplish',\n",
              " 'accomplished',\n",
              " 'accompt',\n",
              " 'accord',\n",
              " 'according',\n",
              " 'accordingly',\n",
              " 'accords',\n",
              " 'accost',\n",
              " 'account',\n",
              " 'accountant',\n",
              " 'accounted',\n",
              " 'accounts',\n",
              " 'accoutred',\n",
              " 'accoutrement',\n",
              " 'accoutrements',\n",
              " 'accursed',\n",
              " 'accurst',\n",
              " 'accusation',\n",
              " 'accusations',\n",
              " 'accusative',\n",
              " 'accuse',\n",
              " 'accused',\n",
              " 'accuser',\n",
              " 'accusers',\n",
              " 'accuses',\n",
              " 'accusing',\n",
              " 'accustom',\n",
              " 'accustomed',\n",
              " 'ace',\n",
              " 'ache',\n",
              " 'acheron',\n",
              " 'aches',\n",
              " 'achieve',\n",
              " 'achieved',\n",
              " 'achievement',\n",
              " 'achievements',\n",
              " 'achilles',\n",
              " 'aching',\n",
              " 'acknowledge',\n",
              " 'acknowledged',\n",
              " 'acorn',\n",
              " 'acquaint',\n",
              " 'acquaintance',\n",
              " 'acquainted',\n",
              " 'acquire',\n",
              " 'acquired',\n",
              " 'acquit',\n",
              " 'acquittance',\n",
              " 'acquitted',\n",
              " 'acre',\n",
              " 'acres',\n",
              " 'across',\n",
              " 'act',\n",
              " 'actaeon',\n",
              " 'acted',\n",
              " 'acting',\n",
              " 'action',\n",
              " 'actions',\n",
              " 'active',\n",
              " 'activity',\n",
              " 'actor',\n",
              " 'actors',\n",
              " 'acts',\n",
              " 'actual',\n",
              " 'acute',\n",
              " 'ad',\n",
              " 'adage',\n",
              " 'adam',\n",
              " 'adamant',\n",
              " 'add',\n",
              " 'added',\n",
              " 'adder',\n",
              " 'adders',\n",
              " 'addicted',\n",
              " 'addiction',\n",
              " 'adding',\n",
              " 'addition',\n",
              " 'additions',\n",
              " 'addle',\n",
              " 'address',\n",
              " 'addrest',\n",
              " 'adds',\n",
              " 'adhere',\n",
              " 'adheres',\n",
              " 'adieu',\n",
              " 'adieus',\n",
              " 'adjacent',\n",
              " 'adjourn',\n",
              " 'adjudged',\n",
              " 'adjunct',\n",
              " 'admirable',\n",
              " 'admiral',\n",
              " 'admiration',\n",
              " 'admire',\n",
              " 'admired',\n",
              " 'admiring',\n",
              " 'admiringly',\n",
              " 'admit',\n",
              " 'admits',\n",
              " 'admittance',\n",
              " 'admitted',\n",
              " 'admonition',\n",
              " 'ado',\n",
              " 'adonis',\n",
              " 'adopt',\n",
              " 'adopted',\n",
              " 'adoption',\n",
              " 'adoration',\n",
              " 'adore',\n",
              " 'adored',\n",
              " 'adores',\n",
              " 'adorn',\n",
              " 'adornment',\n",
              " 'adramadio',\n",
              " 'adrian',\n",
              " 'adriana',\n",
              " 'adriano',\n",
              " 'adulterate',\n",
              " 'adulteress',\n",
              " 'adulterous',\n",
              " 'adultery',\n",
              " 'advance',\n",
              " 'advanced',\n",
              " 'advancement',\n",
              " 'advantage',\n",
              " 'advantageous',\n",
              " 'advantages',\n",
              " 'adventure',\n",
              " 'adventures',\n",
              " 'adventurous',\n",
              " 'adversaries',\n",
              " 'adversary',\n",
              " 'adverse',\n",
              " 'adversity',\n",
              " 'advertise',\n",
              " 'advertised',\n",
              " 'advertisement',\n",
              " 'advice',\n",
              " 'advise',\n",
              " 'advised',\n",
              " 'advisedly',\n",
              " 'advises',\n",
              " 'advocate',\n",
              " 'aedile',\n",
              " 'aediles',\n",
              " 'aegeon',\n",
              " 'aemelia',\n",
              " 'aemilia',\n",
              " 'aemilius',\n",
              " 'aeneas',\n",
              " 'aery',\n",
              " 'aesculapius',\n",
              " 'afar',\n",
              " 'afeard',\n",
              " 'affability',\n",
              " 'affable',\n",
              " 'affair',\n",
              " 'affairs',\n",
              " 'affect',\n",
              " 'affectation',\n",
              " 'affected',\n",
              " 'affecting',\n",
              " 'affection',\n",
              " 'affections',\n",
              " 'affects',\n",
              " 'affiance',\n",
              " 'affianced',\n",
              " 'affined',\n",
              " 'affirm',\n",
              " 'afflict',\n",
              " 'afflicted',\n",
              " 'affliction',\n",
              " 'afflictions',\n",
              " 'afflicts',\n",
              " 'afford',\n",
              " 'affords',\n",
              " 'affright',\n",
              " 'affrighted',\n",
              " 'affrights',\n",
              " 'affront',\n",
              " 'afire',\n",
              " 'afoot',\n",
              " 'afore',\n",
              " 'aforesaid',\n",
              " 'afraid',\n",
              " 'afresh',\n",
              " 'afric',\n",
              " 'after',\n",
              " 'after dinner',\n",
              " 'after love',\n",
              " 'afternoon',\n",
              " 'afterward',\n",
              " 'afterwards',\n",
              " 'again',\n",
              " 'against',\n",
              " 'agamemnon',\n",
              " 'agate',\n",
              " 'age',\n",
              " 'aged',\n",
              " 'agent',\n",
              " 'agents',\n",
              " 'ages',\n",
              " 'aggravate',\n",
              " 'agincourt',\n",
              " 'agitation',\n",
              " 'ago',\n",
              " 'agone',\n",
              " 'agony',\n",
              " 'agree',\n",
              " 'agreed',\n",
              " 'agreeing',\n",
              " 'agreement',\n",
              " 'agrees',\n",
              " 'agrippa',\n",
              " 'ague',\n",
              " 'aguecheek',\n",
              " 'agues',\n",
              " 'ah',\n",
              " 'ai',\n",
              " 'aid',\n",
              " 'aiding',\n",
              " 'aim',\n",
              " 'aiming',\n",
              " 'aims',\n",
              " 'ainsi',\n",
              " 'air',\n",
              " 'airs',\n",
              " 'airy',\n",
              " 'ajax',\n",
              " 'alabaster',\n",
              " 'alack',\n",
              " 'alacrity',\n",
              " 'alarbus',\n",
              " 'alarm',\n",
              " 'alarum',\n",
              " 'alarum bell',\n",
              " 'alarums',\n",
              " 'alas',\n",
              " 'alban',\n",
              " 'albany',\n",
              " 'albeit',\n",
              " 'albion',\n",
              " 'alchemist',\n",
              " 'alcibiades',\n",
              " 'alcides',\n",
              " 'alderman',\n",
              " 'ale',\n",
              " 'ale wife',\n",
              " 'alehouse',\n",
              " 'alencon',\n",
              " 'aleppo',\n",
              " 'ales',\n",
              " 'alexander',\n",
              " 'alexandria',\n",
              " 'alexandrian',\n",
              " 'alexas',\n",
              " 'alias',\n",
              " 'alice',\n",
              " 'alien',\n",
              " 'aliena',\n",
              " 'alighted',\n",
              " 'alike',\n",
              " 'alisander',\n",
              " 'alive',\n",
              " 'all',\n",
              " 'all hail',\n",
              " 'all souls',\n",
              " 'all worthy',\n",
              " 'alla',\n",
              " 'allay',\n",
              " 'allaying',\n",
              " 'alleged',\n",
              " 'allegiance',\n",
              " 'alley',\n",
              " 'alleys',\n",
              " 'alliance',\n",
              " 'allied',\n",
              " 'allies',\n",
              " 'allons',\n",
              " 'allot',\n",
              " 'allow',\n",
              " 'allowance',\n",
              " 'allowed',\n",
              " 'allowing',\n",
              " 'allows',\n",
              " 'allure',\n",
              " 'allusion',\n",
              " 'ally',\n",
              " 'almanac',\n",
              " 'almighty',\n",
              " 'almost',\n",
              " 'alms',\n",
              " 'aloft',\n",
              " 'alone',\n",
              " 'along',\n",
              " 'alonso',\n",
              " 'aloof',\n",
              " 'aloud',\n",
              " 'alps',\n",
              " 'already',\n",
              " 'also',\n",
              " 'altar',\n",
              " 'altars',\n",
              " 'alter',\n",
              " 'alteration',\n",
              " 'altered',\n",
              " 'althaea',\n",
              " 'although',\n",
              " 'altitude',\n",
              " 'altogether',\n",
              " 'alway',\n",
              " 'always',\n",
              " 'am',\n",
              " 'amain',\n",
              " 'amaze',\n",
              " 'amazed',\n",
              " 'amazedly',\n",
              " 'amazedness',\n",
              " 'amazement',\n",
              " 'amazon',\n",
              " 'amazonian',\n",
              " 'ambassador',\n",
              " 'ambassadors',\n",
              " 'amber',\n",
              " 'ambiguities',\n",
              " 'ambition',\n",
              " 'ambitious',\n",
              " 'ambitiously',\n",
              " 'ambles',\n",
              " 'ambling',\n",
              " 'ambush',\n",
              " 'amen',\n",
              " 'amend',\n",
              " 'amended',\n",
              " 'amendment',\n",
              " 'amends',\n",
              " 'amiable',\n",
              " 'amiens',\n",
              " 'amiss',\n",
              " 'amities',\n",
              " 'amity',\n",
              " 'among',\n",
              " 'amongst',\n",
              " 'amorous',\n",
              " 'amort',\n",
              " 'amount',\n",
              " 'amounts',\n",
              " 'ample',\n",
              " 'amplest',\n",
              " 'amplify',\n",
              " 'amply',\n",
              " 'amurath',\n",
              " 'an',\n",
              " 'anatomize',\n",
              " 'anatomized',\n",
              " 'anatomy',\n",
              " 'ancestor',\n",
              " 'ancestors',\n",
              " 'ancestry',\n",
              " 'anchises',\n",
              " 'anchor',\n",
              " 'anchoring',\n",
              " 'anchors',\n",
              " 'ancient',\n",
              " 'ancientry',\n",
              " 'and',\n",
              " 'andrew',\n",
              " 'andromache',\n",
              " 'andronici',\n",
              " 'andronicus',\n",
              " 'anew',\n",
              " 'angel',\n",
              " 'angel like',\n",
              " 'angelo',\n",
              " 'angels',\n",
              " 'anger',\n",
              " 'angerly',\n",
              " 'angers',\n",
              " 'angiers',\n",
              " 'angle',\n",
              " 'angleterre',\n",
              " 'angling',\n",
              " 'anglois',\n",
              " 'angry',\n",
              " 'anguish',\n",
              " 'angus',\n",
              " 'animal',\n",
              " 'animals',\n",
              " 'anjou',\n",
              " 'anne',\n",
              " 'annoy',\n",
              " 'annoyance',\n",
              " 'annual',\n",
              " 'anoint',\n",
              " 'anointed',\n",
              " 'anon',\n",
              " 'another',\n",
              " 'answer',\n",
              " 'answerable',\n",
              " 'answered',\n",
              " 'answering',\n",
              " 'answers',\n",
              " 'ant',\n",
              " 'antenor',\n",
              " 'anthony',\n",
              " 'antiates',\n",
              " 'antic',\n",
              " 'antics',\n",
              " 'antigonus',\n",
              " 'antioch',\n",
              " 'antiochus',\n",
              " 'antipholus',\n",
              " 'antipodes',\n",
              " 'antique',\n",
              " 'antiquity',\n",
              " 'antium',\n",
              " 'antonio',\n",
              " 'antonius',\n",
              " 'antony',\n",
              " 'anvil',\n",
              " 'any',\n",
              " 'anything',\n",
              " 'apace',\n",
              " 'apart',\n",
              " 'ape',\n",
              " 'apemantus',\n",
              " 'apes',\n",
              " 'apiece',\n",
              " 'apish',\n",
              " 'apollo',\n",
              " 'apology',\n",
              " 'apoplexy',\n",
              " 'apothecary',\n",
              " 'appal',\n",
              " 'appals',\n",
              " 'apparel',\n",
              " 'apparell',\n",
              " 'apparent',\n",
              " 'apparition',\n",
              " 'appeach',\n",
              " 'appeal',\n",
              " 'appear',\n",
              " 'appearance',\n",
              " 'appeared',\n",
              " 'appeareth',\n",
              " 'appearing',\n",
              " 'appears',\n",
              " 'appease',\n",
              " 'appeased',\n",
              " 'appelez vous',\n",
              " 'appellant',\n",
              " 'appertain',\n",
              " 'appertaining',\n",
              " 'appertinent',\n",
              " 'appetite',\n",
              " 'appetites',\n",
              " 'applaud',\n",
              " 'applauding',\n",
              " 'applause',\n",
              " 'apple',\n",
              " 'apple john',\n",
              " 'apple johns',\n",
              " 'apples',\n",
              " 'appliance',\n",
              " 'appliances',\n",
              " 'applied',\n",
              " 'apply',\n",
              " 'appoint',\n",
              " 'appointed',\n",
              " 'appointment',\n",
              " 'appointments',\n",
              " 'apprehend',\n",
              " 'apprehended',\n",
              " 'apprehends',\n",
              " 'apprehension',\n",
              " 'apprehensions',\n",
              " 'apprehensive',\n",
              " 'approach',\n",
              " 'approaches',\n",
              " 'approacheth',\n",
              " 'approaching',\n",
              " 'approbation',\n",
              " 'approof',\n",
              " 'approve',\n",
              " 'approved',\n",
              " 'approves',\n",
              " 'apricocks',\n",
              " 'april',\n",
              " 'apron',\n",
              " 'aprons',\n",
              " 'apt',\n",
              " 'apter',\n",
              " 'aptly',\n",
              " 'aptness',\n",
              " 'aqua',\n",
              " 'aqua vitae',\n",
              " 'aquitaine',\n",
              " 'arabia',\n",
              " 'arabian',\n",
              " 'arbitrate',\n",
              " 'arbitrator',\n",
              " 'arbitrement',\n",
              " 'arbour',\n",
              " 'arc',\n",
              " 'arch',\n",
              " 'arch villain',\n",
              " 'archbishop',\n",
              " 'arched',\n",
              " 'archer',\n",
              " 'archers',\n",
              " 'archery',\n",
              " 'archidamus',\n",
              " 'arden',\n",
              " 'ardour',\n",
              " 'are',\n",
              " 'argal',\n",
              " 'argier',\n",
              " 'argosies',\n",
              " 'argosy',\n",
              " 'argue',\n",
              " 'argued',\n",
              " 'argues',\n",
              " 'arguing',\n",
              " 'argument',\n",
              " 'arguments',\n",
              " 'argus',\n",
              " 'ariadne',\n",
              " 'ariel',\n",
              " 'aright',\n",
              " 'arise',\n",
              " 'aristotle',\n",
              " 'arithmetic',\n",
              " 'arm',\n",
              " 'armado',\n",
              " 'armagnac',\n",
              " 'armed',\n",
              " 'armenia',\n",
              " 'armies',\n",
              " 'armigero',\n",
              " 'arming',\n",
              " 'armipotent',\n",
              " 'armour',\n",
              " 'armourer',\n",
              " 'armourers',\n",
              " 'armours',\n",
              " 'arms',\n",
              " 'army',\n",
              " 'aroint',\n",
              " 'arose',\n",
              " 'arragon',\n",
              " 'arraign',\n",
              " 'arrant',\n",
              " 'arras',\n",
              " 'array',\n",
              " 'arrest',\n",
              " 'arrested',\n",
              " 'arrests',\n",
              " 'arrival',\n",
              " 'arrive',\n",
              " 'arrived',\n",
              " 'arrives',\n",
              " 'arrogance',\n",
              " 'arrogant',\n",
              " 'arrow',\n",
              " 'arrows',\n",
              " 'art',\n",
              " 'artemidorus',\n",
              " 'arthur',\n",
              " 'article',\n",
              " 'articles',\n",
              " 'articulate',\n",
              " 'artificial',\n",
              " 'artillery',\n",
              " 'artist',\n",
              " 'arts',\n",
              " 'arviragus',\n",
              " 'as',\n",
              " 'ascend',\n",
              " 'ascended',\n",
              " 'ascends',\n",
              " 'ascension day',\n",
              " 'ascribe',\n",
              " 'ashamed',\n",
              " 'ashes',\n",
              " 'ashford',\n",
              " 'ashore',\n",
              " 'asia',\n",
              " 'aside',\n",
              " 'ask',\n",
              " 'asked',\n",
              " 'asketh',\n",
              " 'asking',\n",
              " 'asks',\n",
              " 'asleep',\n",
              " 'aspect',\n",
              " 'aspects',\n",
              " 'aspic',\n",
              " 'aspire',\n",
              " 'aspired',\n",
              " 'aspiring',\n",
              " 'ass',\n",
              " 'assail',\n",
              " 'assailed',\n",
              " 'assailing',\n",
              " 'assails',\n",
              " 'assault',\n",
              " 'assaults',\n",
              " 'assay',\n",
              " 'assays',\n",
              " 'assemble',\n",
              " 'assembled',\n",
              " 'assemblies',\n",
              " 'assembly',\n",
              " 'assent',\n",
              " 'asses',\n",
              " 'assign',\n",
              " 'assigns',\n",
              " 'assist',\n",
              " 'assistance',\n",
              " 'assistant',\n",
              " 'assistants',\n",
              " 'assisted',\n",
              " 'associate',\n",
              " 'assume',\n",
              " 'assumes',\n",
              " 'assurance',\n",
              " 'assure',\n",
              " 'assured',\n",
              " 'assuredly',\n",
              " 'assyrian',\n",
              " 'astonish',\n",
              " 'astraea',\n",
              " 'astray',\n",
              " 'asunder',\n",
              " 'at',\n",
              " 'ate',\n",
              " 'ates',\n",
              " 'athenian',\n",
              " 'athenians',\n",
              " 'athens',\n",
              " 'athversary',\n",
              " 'athwart',\n",
              " 'atomies',\n",
              " 'atone',\n",
              " 'atonement',\n",
              " 'attach',\n",
              " 'attached',\n",
              " 'attain',\n",
              " 'attainder',\n",
              " 'attaint',\n",
              " 'attainted',\n",
              " 'attempt',\n",
              " 'attempted',\n",
              " 'attempting',\n",
              " 'attempts',\n",
              " 'attend',\n",
              " 'attendance',\n",
              " 'attendant',\n",
              " 'attendants',\n",
              " 'attended',\n",
              " 'attending',\n",
              " 'attends',\n",
              " 'attent',\n",
              " 'attention',\n",
              " 'attentive',\n",
              " 'attest',\n",
              " 'attire',\n",
              " 'attired',\n",
              " 'attires',\n",
              " 'attorney',\n",
              " 'attorneys',\n",
              " 'attractions',\n",
              " 'attractive',\n",
              " 'attracts',\n",
              " 'attribute',\n",
              " 'attributes',\n",
              " 'au',\n",
              " 'auburn',\n",
              " 'audacious',\n",
              " 'audible',\n",
              " 'audience',\n",
              " 'audit',\n",
              " 'auditor',\n",
              " 'audrey',\n",
              " 'aufidius',\n",
              " 'aught',\n",
              " 'augment',\n",
              " 'augmented',\n",
              " 'augmenting',\n",
              " 'augurer',\n",
              " 'augurers',\n",
              " 'augury',\n",
              " 'august',\n",
              " 'augustus',\n",
              " 'aumerle',\n",
              " 'aunchient',\n",
              " 'aunt',\n",
              " 'aurora',\n",
              " 'auspicious',\n",
              " 'aussi',\n",
              " 'austere',\n",
              " 'austerely',\n",
              " 'austerity',\n",
              " 'austria',\n",
              " 'authentic',\n",
              " 'author',\n",
              " 'authorities',\n",
              " 'authority',\n",
              " 'authors',\n",
              " 'autolycus',\n",
              " 'autumn',\n",
              " 'auvergne',\n",
              " 'avail',\n",
              " 'avails',\n",
              " 'avarice',\n",
              " 'avaunt',\n",
              " 'ave maries',\n",
              " 'avenged',\n",
              " 'avez',\n",
              " 'avoid',\n",
              " 'avoided',\n",
              " 'avouch',\n",
              " 'avouches',\n",
              " 'avow',\n",
              " 'await',\n",
              " 'awake',\n",
              " 'awaked',\n",
              " 'awaken',\n",
              " 'awakes',\n",
              " 'awaking',\n",
              " 'awards',\n",
              " 'away',\n",
              " 'awe',\n",
              " 'aweary',\n",
              " 'aweless',\n",
              " 'awful',\n",
              " 'awhile',\n",
              " 'awkward',\n",
              " 'awl',\n",
              " 'awry',\n",
              " 'axe',\n",
              " 'axle tree',\n",
              " 'ay',\n",
              " 'aye',\n",
              " 'azured',\n",
              " 'b',\n",
              " 'ba',\n",
              " 'babble',\n",
              " 'babbling',\n",
              " 'babe',\n",
              " 'babes',\n",
              " 'babies',\n",
              " 'baboon',\n",
              " 'baby',\n",
              " 'babylon',\n",
              " 'bacchus',\n",
              " 'bachelor',\n",
              " 'bachelors',\n",
              " 'back',\n",
              " 'backing',\n",
              " 'backs',\n",
              " 'backward',\n",
              " 'bacon',\n",
              " 'bad',\n",
              " 'bade',\n",
              " 'badge',\n",
              " 'badges',\n",
              " 'badness',\n",
              " 'baes',\n",
              " 'baffle',\n",
              " 'baffled',\n",
              " 'bag',\n",
              " 'baggage',\n",
              " 'bagot',\n",
              " 'bagpipe',\n",
              " 'bags',\n",
              " 'bail',\n",
              " 'bait',\n",
              " 'baited',\n",
              " 'baiting',\n",
              " 'baits',\n",
              " 'baked',\n",
              " 'balance',\n",
              " 'bald',\n",
              " 'baleful',\n",
              " 'balk',\n",
              " 'ball',\n",
              " 'ballad',\n",
              " 'ballad makers',\n",
              " 'ballads',\n",
              " 'balls',\n",
              " 'balm',\n",
              " 'balmy',\n",
              " 'balthasar',\n",
              " 'balthazar',\n",
              " 'ban',\n",
              " 'band',\n",
              " 'bandit',\n",
              " 'banditti',\n",
              " 'bands',\n",
              " 'bandy',\n",
              " 'bandying',\n",
              " 'bane',\n",
              " 'bang',\n",
              " 'banish',\n",
              " 'banished',\n",
              " 'banishment',\n",
              " 'bank',\n",
              " 'bankrupt',\n",
              " 'banks',\n",
              " 'banner',\n",
              " 'banners',\n",
              " 'banns',\n",
              " 'banquet',\n",
              " 'banqueting',\n",
              " 'banquets',\n",
              " 'banquo',\n",
              " 'bans',\n",
              " 'baptism',\n",
              " 'baptista',\n",
              " 'bar',\n",
              " 'barbara',\n",
              " 'barbarian',\n",
              " 'barbarism',\n",
              " 'barbarous',\n",
              " 'barbary',\n",
              " 'barbason',\n",
              " 'barber',\n",
              " 'bard',\n",
              " 'bardolph',\n",
              " 'bare',\n",
              " 'bare foot',\n",
              " 'bare headed',\n",
              " 'bared',\n",
              " 'barefoot',\n",
              " 'barely',\n",
              " 'bareness',\n",
              " 'bargain',\n",
              " 'bargains',\n",
              " 'barge',\n",
              " 'bark',\n",
              " 'barking',\n",
              " 'barks',\n",
              " 'barm',\n",
              " 'barn',\n",
              " 'barnardine',\n",
              " 'barne',\n",
              " 'barnes',\n",
              " 'barnet',\n",
              " 'barns',\n",
              " 'barons',\n",
              " 'barr',\n",
              " 'barren',\n",
              " 'barricado',\n",
              " 'bars',\n",
              " 'base',\n",
              " 'base born',\n",
              " 'basely',\n",
              " 'baseness',\n",
              " 'baser',\n",
              " 'bases',\n",
              " 'basest',\n",
              " 'bashful',\n",
              " 'basilisk',\n",
              " 'basilisks',\n",
              " 'basin',\n",
              " 'basis',\n",
              " 'basket',\n",
              " 'bass',\n",
              " 'bassanio',\n",
              " 'basset',\n",
              " 'bassianus',\n",
              " 'bastard',\n",
              " 'bastards',\n",
              " 'bastardy',\n",
              " 'bastinado',\n",
              " 'basting',\n",
              " 'bat',\n",
              " 'bate',\n",
              " 'bated',\n",
              " 'bates',\n",
              " 'bath',\n",
              " 'bathe',\n",
              " 'bathed',\n",
              " 'bats',\n",
              " 'batten',\n",
              " 'batter',\n",
              " 'battering',\n",
              " 'batters',\n",
              " 'battery',\n",
              " 'battle',\n",
              " 'battlements',\n",
              " 'battles',\n",
              " 'bauble',\n",
              " 'bawcock',\n",
              " 'bawd',\n",
              " 'bawdry',\n",
              " 'bawds',\n",
              " 'bawdy',\n",
              " 'bawdy house',\n",
              " 'bawdy houses',\n",
              " 'bay',\n",
              " 'baynard',\n",
              " 'be',\n",
              " 'beach',\n",
              " 'beached',\n",
              " 'beacon',\n",
              " ...]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fcagq-X-cLsh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2TNSiwj_lYT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ffZReCmNBMed",
        "colab_type": "code",
        "outputId": "129aef72-34cf-4836-85d2-c0232088bc0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "beam_search(ng3_shk, 5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'goodwin swinstead melun edmundsbury bigot '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WXvqdBk4BPF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = {1: 2, 3: 4, 4: 3, 2: 1, 0: 0}\n",
        "sorted_x = sorted(x.items(), key=lambda kv: kv[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkZQmxsBE44N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import collections\n",
        "\n",
        "sorted_dict = collections.OrderedDict(sorted_x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-eTRWuxE8MK",
        "colab_type": "code",
        "outputId": "bbf576ff-a8db-4945-c05d-eea8608e55c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "list(sorted_dict.keys())[-3:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 4, 3]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzwQE1Z4E-My",
        "colab_type": "code",
        "outputId": "f8887e16-a60d-46d4-951d-a63a99064952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "sorted_dict.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "odict_keys([0, 2, 1, 4, 3])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTKO-uboFOaM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}