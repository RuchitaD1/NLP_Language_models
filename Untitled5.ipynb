{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled5.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuchitaD1/NLP_Language_models/blob/master/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8nI8kQjOEsu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NGramLM:\n",
        "  def __init__(self, n):\n",
        "        self.n = n\n",
        "        self.ngram_counts = {}\n",
        "        self.context_counts = {}\n",
        "        self.vocabulary = {}\n",
        "        \n",
        "        \n",
        "  def update(self, text):\n",
        "      words = text.split()\n",
        "      arr = []\n",
        "      for word in words:\n",
        "          if word in self.vocabulary:\n",
        "              self.vocabulary[word] = self.vocabulary[word]+1\n",
        "          else:\n",
        "              self.vocabulary[word] = 1\n",
        "          arr.append(word)\n",
        "      ngrams = []\n",
        "      for tg in get_ngrams(n, arr):\n",
        "          ngrams.append(tg)          \n",
        "      for val in ngrams:\n",
        "        word = val[self.n-1]\n",
        "        context = tuple(val[:self.n-1])\n",
        "        if context in model.context_counts:\n",
        "            model.context_counts[context] = model.context_counts[context] + 1\n",
        "        else:\n",
        "            model.context_counts[context] = 1\n",
        "        \n",
        "        if val in model.ngram_counts:\n",
        "          model.ngram_counts[val] = model.ngram_counts[val] + 1\n",
        "        else:\n",
        "          model.ngram_counts[val] = 1 \n",
        "\n",
        "          \n",
        "          \n",
        "  def word_prob(self, word, context):\n",
        "    if context not in self.context_counts:\n",
        "      return 1.0/float(len(self.vocabulary))\n",
        "    c_context = self.context_counts[context]\n",
        "    bc_c = context\n",
        "    l_b_c=list(bc_c)\n",
        "    l_b_c.append(word)\n",
        "    ng = tuple(l_b_c)\n",
        "    c_ngram = self.ngram_counts[ng]\n",
        "    \n",
        "    wp = float(c_ngram)/float(c_context)\n",
        "    print(wp)\n",
        "    return wp\n",
        "                       \n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2g2cUJSRDGH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ngrams(n, arr):\n",
        "  for i in range(len(arr)-n-1):\n",
        "    temp_gram = []\n",
        "    for j in range(n):\n",
        "        temp_gram.append(arr[i+j])\n",
        "    yield temp_gram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WnRPInUQOpgZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_ngramlm(n, corpus_path):\n",
        "  arr = []\n",
        "  f = open(corpus_path, 'r')\n",
        "  for i in range(n-1):\n",
        "      arr.append('<s>')\n",
        "  x=f.readlines()\n",
        "  model = NGramLM(n)\n",
        "  for line in x:\n",
        "      words = line.split()\n",
        "      for word in words:\n",
        "          if word in model.vocabulary:\n",
        "              model.vocabulary[word] = model.vocabulary[word]+1\n",
        "          else:\n",
        "              model.vocabulary[word] = 1\n",
        "          arr.append(word)\n",
        "  ngrams = []\n",
        "  for tg in get_ngrams(n, arr):\n",
        "      ngrams.append(tg)          \n",
        "  for val in ngrams:\n",
        "    \n",
        "    word = val[n-1]\n",
        "    context = tuple(val[:n-1])\n",
        "    if context in model.context_counts:\n",
        "        model.context_counts[context] = model.context_counts[context] + 1\n",
        "    else:\n",
        "        model.context_counts[context] = 1\n",
        "    val1 = tuple(val)\n",
        "    if val1 in model.ngram_counts:\n",
        "      model.ngram_counts[val1] = model.ngram_counts[val1] + 1\n",
        "    else:\n",
        "      model.ngram_counts[val1] = 1\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4dRg_3WVSIK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = create_ngramlm(3, 'warpeace.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv7at71sO1Nq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "def text_prob(model, text):\n",
        "  words = text.split()\n",
        "  arr = []\n",
        "  for word in words:\n",
        "    arr.append(word)\n",
        "  p = 0.0\n",
        "  for ng in get_ngrams(model.n, arr):\n",
        "    wd = ng[model.n-1]\n",
        "    ctx = tuple(ng[:model.n-1])\n",
        "    wp = model.word_prob(wd, ctx)\n",
        "    p+= math.log(wp)\n",
        "  return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tll3gP6aWPCw",
        "colab_type": "code",
        "outputId": "12beee5c-6181-4b76-a2eb-f142bc97dcd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "text_prob(model, 'Where is the prince, my Dauphin?')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-70109f276937>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Where is the prince, my Dauphin?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-74-5788162f8a13>\u001b[0m in \u001b[0;36mtext_prob\u001b[0;34m(model, text)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mng\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mng\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mwp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mp\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-da5c22ef96e8>\u001b[0m in \u001b[0;36mword_prob\u001b[0;34m(self, word, context)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0ml_b_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mng\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_b_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mc_ngram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mngram_counts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mng\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mwp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_ngram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: ('Where', 'is', 'the')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W832F3TsWoGV",
        "colab_type": "code",
        "outputId": "4ffee0db-7cb6-4612-9d9d-9881972a1fda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVu58i9PZTWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}